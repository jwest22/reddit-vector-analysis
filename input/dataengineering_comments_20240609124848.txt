{"body": "Thanks for the detailed reply", "author": "Standard-Alfalfa6501", "created_utc": "2024-06-09 12:45:12", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7t3ey7/"}
{"body": "Other things to look out for, for example, could be how duckdb performs within containers. I've run some workloads on Kubernetes where using duckdb's ability to limit e.g. memory or number of threads becomes challenging (there are some open issues about that). Or tricks like using \"preserve_insertion_order\", like they explain in their tuning guide. Good job anyway, and long live to duckdb :)", "author": "stefanondisponibile", "created_utc": "2024-06-09 12:41:05", "permalink": "/r/dataengineering/comments/1dbrb67/data_pipelines_with_duckdb/l7t2vj7/"}
{"body": "I worked in the industry for over a year. I am self taught in data analytics.\n\nI can't recall hearing about IBM certifications. \n\nSkills you may want to learn will include AWS, DBT (Data Build Tool), Snowflake, etc. Python and SQL are of course important foundations.\n\nAn impressive portfolio is one of the most important things.", "author": "gregTheEye", "created_utc": "2024-06-09 12:35:32", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7t26ck/"}
{"body": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "author": "AutoModerator", "created_utc": "2024-06-09 12:34:18", "permalink": "/r/dataengineering/comments/1dbsprj/most_interesting_data_blogs_to_read/l7t20k4/"}
{"body": "all facts", "author": "stefanondisponibile", "created_utc": "2024-06-09 12:27:00", "permalink": "/r/dataengineering/comments/1dboo9l/2010_2017_ml_pip_install_scikitlearn_2017_2023_ml/l7t14c9/"}
{"body": "Any recommendations for all of those you mentioned?", "author": "Commercial-Ask971", "created_utc": "2024-06-09 12:25:31", "permalink": "/r/dataengineering/comments/1d9y1ao/what_swe_skills_do_i_need_to_lvl_up/l7t0yci/"}
{"body": "Generally yes, you can use file system as KV store, but performance will be magnitudes slower than a real KV stores due to multiple reasons.\n\nFor single machine use case, a dedicated disk-based KV store like RocksDB will perform much better.\n\nFor multi machine use case, self hosting Cassandra/ScyllaDB/HBase can be a cost effective solution as well.", "author": "random_lonewolf", "created_utc": "2024-06-09 12:23:34", "permalink": "/r/dataengineering/comments/1db88q6/key_value_conundrum/l7t0pug/"}
{"body": "Your post/comment was removed because it violated rule #3 (Do a search before asking a question). The question you asked has been answered in the wiki so we remove these questions to keep the feed digestable for everyone.", "author": "dataengineering-ModTeam", "created_utc": "2024-06-09 12:11:54", "permalink": "/r/dataengineering/comments/1dbr8ll/transitioning_from_a_bi_analystdev_role_to_a_de/l7szexz/"}
{"body": "I think it\u2019s a great plan to get a breadth of fundamental skills. Others are giving good ideas for fine-tuning the strategy. I want to share what I have seen candidates do, when reviewing applications for an entry-level position for those who lack professional experience. Some resumes arranged personal or school projects the way others would arrange work history. In fact, sometimes I felt this gave me an even better understanding of their relevant skills than the work history of others. Those commenters who have recommended building a portfolio of personal projects are giving solid advice, I think. This will give you a chance to apply the skills you are learning in the context of a problem.\n\nIt\u2019s a tough market out there, but you are clearly motivated to succeed.  Keep up the good work, and best of luck!", "author": "LowDownAndShwifty", "created_utc": "2024-06-09 12:11:35", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7szdpo/"}
{"body": "So, I'm not experienced, so take my other comment with a grain of salt, okay? It would probably be best to ask some seniors and stuff, but this was my reality of getting hired in 2024.\n\nI think they are really looking for people who not only got the basics down but are genuinely into learning. I talk passionately about my projects, and it catches the interviewers' attention (my manager actually said that after I was hired).", "author": "tinycockatoo", "created_utc": "2024-06-09 12:07:01", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7syvna/"}
{"body": "I had some python experience, a tiny bit of R and a little bit of matlab. The company that hired me mostly used R for their setup. They gave me a small take home assignment that I solved in python. It wasn't anything big, basically just fitting a function to some data. My solution wasn't perfect but showed that I could figure shit out on my own and they hired me.\n\nLooking back, that company was actually a pretty rinky dink operation, but I learned a lot. One of my colleagues was very competent and helped me pick up a lot of fundamentals of data engineering.\n\nAs for skillset, it was an understanding of basic data concepts (transformations, structures etc) and I think most importantly being able to show you can figure stuff out independently. I don't want to sound too full of myself, but if I can learn physics, I'm pretty sure I can easily learn whatever some company is doing.", "author": "ilikedmatrixiv", "created_utc": "2024-06-09 12:05:15", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7syoqf/"}
{"body": "once you start working .. you'll get more clarity.. so far you are doing all the right things .. go on", "author": "ab624", "created_utc": "2024-06-09 12:04:39", "permalink": "/r/dataengineering/comments/1dbr8ll/transitioning_from_a_bi_analystdev_role_to_a_de/l7symbf/"}
{"body": "Currently, we do not allow job postings in this subreddit. Please use r/dataengineeringjobs instead.", "author": "dataengineering-ModTeam", "created_utc": "2024-06-09 12:03:58", "permalink": "/r/dataengineering/comments/1dbmc5i/looking_for_a_remote_job_as_senior_de/l7syjn6/"}
{"body": "Speaking for myself (got hired as a jr DE this year): relational databases and all their fundamental concepts; data warehouse and modeling; difference between batch and streaming; be able to namedrop some services/products and what they are used for; minimal contact with a big cloud provider (I did a big college project using my Azure free credits); machine learning basic concepts (idk why exactly); and the most important: SQL!!! LOTS of SQL concept questions.\n\nThey also asked me about Pyspark and POO, but I was honest about the fact that I didn't know much about it.\n\nAll of this I learned at college. I think the best way to learn is through end-to-end data engineering projects, using the free cloud credits that Azure, AWS and GCP offer to students.\n\nYou can start by taking a big table from a Kaggle dataset and modeling a data warehouse by normalizing it. Use Postgres to create the data warehouse using SQL (all the tables and relationships). Create a Python (pandas is usually enough) script that takes some data from the cloud, does the necessary transformations, and loads the data into the DW. Now, think about some cool business questions you could answer about the dataset. Connect your DW to a dashboard-making thing (Tableau, Power BI, doesn't matter), and create some visualizations.\n\nNow you have a cool project you can put it in your resume and talk about in interviews.\n\nA second, even cooler project, is to create a data lakehouse. You can do this easily using Databricks Community (free) + some cloud provider. You need to understand the medallion architecture. Create scripts that will take raw data from the cloud, transform it, and load in the bronze, silver, and gold layers. Connect your gold layer to the dashboard tool of your choice and do a cool dashboard. Connect your silver layer to a Jupyter notebook and do some basic machine learning analysis. Do all the code in Pyspark. The syntax is very similar to pandas, and in Databricks, you shouldn't really worry about setting up Spark clusters and stuff.\n\nI think doing this and actually UNDERSTANDING what you're doing should be enough. Your basic data concepts game should be very strong, so this is the first step if you're not there yet.\n\nIf you already did all of this and are not getting interviews, this sucks, I know the market is tough rn. Luck plays a big role. Always try to customize your resume to the role, using the keywords present; you might not even be passing the initial AI filter. ChatGPT is useful for this: you can give it a job description and ask him to make a fake resume of the ideal candidate, them try to use this template for yours.", "author": "tinycockatoo", "created_utc": "2024-06-09 11:59:52", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7sy3o9/"}
{"body": "Beware: if you want to work for companies in the US duble taxation could be an issue", "author": "Agreeable_Bake_783", "created_utc": "2024-06-09 11:53:56", "permalink": "/r/dataengineering/comments/1dbmc5i/looking_for_a_remote_job_as_senior_de/l7sxgo0/"}
{"body": "We are using redshift and actively researching iceberg with starburst or athena as a potential replacement. \u00a0I don\u2019t have experience with snowflake. Our redshift schemas are built on the fly for additive changes via script generation on the daily or monthly parquet files we ingest. \u00a0We detect breaking changes and require a cutover when those occur, but they are planned.", "author": "vandelay82", "created_utc": "2024-06-09 11:49:57", "permalink": "/r/dataengineering/comments/1dbh929/terraform_vs_opentofu/l7sx1m6/"}
{"body": "Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "author": "AutoModerator", "created_utc": "2024-06-09 11:05:50", "permalink": "/r/dataengineering/comments/1dbr8ll/transitioning_from_a_bi_analystdev_role_to_a_de/l7ssnt6/"}
{"body": "Can you point out the skillset you had when you were selected for the junior role? On job descriptions it seems like a bit broader and too much for a starter role.It would have been nice to get a realistic comment from someone who actually started as a junior DE", "author": "Standard-Alfalfa6501", "created_utc": "2024-06-09 10:56:13", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7srsbg/"}
{"body": "I never had a formal education in DE. I have a degree in physics and did some programming during my studies, most of which was self taught. I then started in a junior role and learned everything on the job. I'm a bad reference for official resources.", "author": "ilikedmatrixiv", "created_utc": "2024-06-09 10:51:15", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7src67/"}
{"body": "Where can we achieve maximum understanding of basics? Can you share any resources like standard textbooks or courses?", "author": "Standard-Alfalfa6501", "created_utc": "2024-06-09 10:42:15", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7sqjm2/"}
{"body": "You get 400eur in free credits from microsoft for a month and many resources have free trials. However, there is always a risk with personal cloud subscriptions of spinning up something really expensive and noticing.", "author": "Annual_Scratch7181", "created_utc": "2024-06-09 10:35:09", "permalink": "/r/dataengineering/comments/1dbqca5/azure_data_engineering_subscription/l7spxic/"}
{"body": "The basic cloud certifications are kind of useless in my opinion. I have some of them (AWS and Azure) and if I would hire someone, the presence of said certs would not act as a benefit for me personally. If the applicant would emphasize the certs, it would even be a deterrent.\n\nThey don't test your skills or knowledge as a DE, they test how well you know the different products of the cloud provider. I am unfamiliar with IBM certs, so I can't comment on their usefulness.\n\nFor me personally, what I would look for in a junior is understanding of important concepts. E.g. how certain transformations work, how they would use different datasets to get certain information out of them, the concept of normalization etc. If you understand the basics, putting them into practice isn't all that difficult.", "author": "ilikedmatrixiv", "created_utc": "2024-06-09 10:26:52", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7sp86y/"}
{"body": "I think having a personal blog and some projects on github that you put in your CV will maximise the chances to get interviews.", "author": "youngnight1", "created_utc": "2024-06-09 10:06:11", "permalink": "/r/dataengineering/comments/1dbpj3i/is_it_possible_to_become_a_data_engineer_in_6/l7sniim/"}
{"body": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "author": "AutoModerator", "created_utc": "2024-06-09 10:04:02", "permalink": "/r/dataengineering/comments/1dbqca5/azure_data_engineering_subscription/l7snc4o/"}
{"body": "With 7 years of data analysts experience I don\u2019t think it will be difficult to transition into DE. With your current role you must to familiar with DHW concepts (if not then you have a problem), along with it you need to up skill in \n- data engineering concepts \n- data pipeline architecture \n- anyone of programming languages that support advance data engineering (Java, python, scala,go) and python is easy to start. \n- Cloud technologies and containers is good but a must.\n- understanding of GIT\n- do some hands on project and show it in your profile.\n\nHere are some basics that I wrote, hope it helps you. Good luck.\n\nhttps://medium.com/@syedkadaransari/demystifying-data-pipelines-architecture-types-design-patterns-and-recommendations-part-1-107d0fd58202\n\nhttps://medium.com/@syedkadaransari/demystifying-data-pipelines-architecture-types-design-patterns-and-recommendations-part-2-856846038ecc\n\nhttps://medium.com/@syedkadaransari/data-warehousing-best-practices-principles-for-designing-effective-data-storage-systems-e8093814a14e\n\nhttps://medium.com/@syedkadaransari/7-things-i-wish-i-had-known-as-a-junior-data-engineer-9686c4f66251", "author": "OmuaOmua", "created_utc": "2024-06-09 08:53:05", "permalink": "/r/dataengineering/comments/1da5ztr/any_tips_for_breaking_into_the_field/l7shmrb/"}
{"body": "I think your range is reasonable. Although it depends if you are looking for a permanent position or contracting position and what country.... I have found contracting to be far more lucrative until the economy shit itself recently, and funding is getting cut.", "author": "Bootlegcrunch", "created_utc": "2024-06-09 08:41:02", "permalink": "/r/dataengineering/comments/1dbmc5i/looking_for_a_remote_job_as_senior_de/l7sgo3m/"}
{"body": "You can already see that [here](https://github.com/KamilKolanowski/rick_morty_api_analysis) :)", "author": "tanssive", "created_utc": "2024-06-09 08:35:36", "permalink": "/r/dataengineering/comments/1d9rtes/rick_and_morty_data_analysis_with_polars/l7sg8cj/"}
{"body": "Meta uses Spark and ORC in their analytics space. Of course some SQL tools for sure as well.\n\nhttps://medium.com/@AnalyticsAtMeta/data-engineering-at-meta-high-level-overview-of-the-internal-tech-stack-a200460a44fe", "author": "dustinBKK", "created_utc": "2024-06-09 08:28:43", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7sfogl/"}
{"body": "original post: [https://www.linkedin.com/posts/julienhuraultanalytics\\_2010-2017-ml-pip-install-scikit-learn-activity-7203719665326419969-xuKc](https://www.linkedin.com/posts/julienhuraultanalytics_2010-2017-ml-pip-install-scikit-learn-activity-7203719665326419969-xuKc)", "author": "Economy-Spread1955", "created_utc": "2024-06-09 08:05:57", "permalink": "/r/dataengineering/comments/1dboo9l/2010_2017_ml_pip_install_scikitlearn_2017_2023_ml/l7sdtbm/"}
{"body": "I mean, you could count adversarial search as \u201cAI\u201d, it doesn\u2019t include evaluating probabilities. Albeit it\u2019s very basic, but I\u2019d still count it as AI", "author": "pioverpie", "created_utc": "2024-06-09 07:57:47", "permalink": "/r/dataengineering/comments/1d9s2qe/what_are_everyones_hot_takes_with_some_of_the/l7sd4xd/"}
{"body": "In\u200b my\u200b country, seems\u200b DE\u200b can\u200b not\u200b be\u200b enought", "author": "Icy-Ad-6789", "created_utc": "2024-06-09 07:39:37", "permalink": "/r/dataengineering/comments/1b7fcgo/is_everyone_becoming_a_data_engineer_and_is_it/l7sbmre/"}
{"body": "I would say if you chunk your data into reasonable partitions based on a formula for a key you could make it  fast", "author": "Sufficient_Example30", "created_utc": "2024-06-09 07:29:20", "permalink": "/r/dataengineering/comments/1db88q6/key_value_conundrum/l7sarpn/"}
{"body": "Not 100% related to Databricks cost but spinning off 100+ jobs is probably incurring elevated VMs cost. Have you considered running multiple jobs within the same cluster ? Magic commands will help to execute py files or notebooks within another file/notebook.", "author": "RoundReveal", "created_utc": "2024-06-09 07:03:38", "permalink": "/r/dataengineering/comments/1dae1uz/how_did_you_reduce_your_databricks_costs/l7s8kp1/"}
{"body": "Yeah, the main reason I split it into two tables instead of leaving it as one and relying on compression is because I wanted the metadata history to be stored separately from the stats history. \n\nThis way you can easily query the metadata history table and just see rows if the index has ever been altered or changed...Things like fill factor, padding, disabled, etc. Whereas the stats history table will have multiple records per day going back about 6 months for the same index.\n\nIt does make things a bit difficult because now you can't easily align index metadata status with stats history records, but that's really not much of a concern. \n\nAs far as the row groups, it seems to be ok. The process runs about twice a day where it inserts roughly 7,500 records 450 times in the span of a few minutes. And SQL Server relies on those row groups to perform the auto deletes from the history table. \n\nPerformance seems to be pretty good though. This isn't the type of system where queries need to return within a second for an app or anything. So as long as they are running within a minute or so, I'm not going to bother fine tuning it or doing any huge refactoring.", "author": "chadbaldwin", "created_utc": "2024-06-09 06:54:55", "permalink": "/r/dataengineering/comments/1d92cm0/discussion_i_built_a_tool_for_analyzing_sql/l7s7tgl/"}
{"body": "I'll have to look into that. I'm not too familiar with time series databases. I've messed around with Prometheus and Grafana just a tiny bit years ago with playing with Pi-Hole. I've also heard of InfluxDB, but never used it. Though a lot of people use it for Home Assistant, so maybe I'll migrate my smart home stuff over to learn how to use it.\n\nI suppose technically you could say that Splunk is a time series database which supports direct JSON loading.\n\nI did try to build this originally with Splunk, but unfortunately it just didn't scale well.\n\nThat said, due to how SQL Server stores these statistics, I would assume there's no way around the pre-processing, right? \n\nFor example...SQL Server stores the number of read operations per index as a counter. That counter continues to go up forever and only ever resets when the service/host restarts, or the database is restored (and a couple other weird cases).\n\nSo my current system calculates a best guess approximation of how old the statistics are, because SQL server doesn't tell you. And then it figures out whether to calculate the delta (difference between the two snapshots), or to take the stats as-is, because the stats were reset at some point since the previous snapshot, so there's no delta to calculate.\n\nIf you don't pre-process, then you have to do all that on the fly as a streaming function, which seems like a waste of compute power.", "author": "chadbaldwin", "created_utc": "2024-06-09 06:46:54", "permalink": "/r/dataengineering/comments/1d92cm0/discussion_i_built_a_tool_for_analyzing_sql/l7s73tc/"}
{"body": "You\u2019re comparing yourself to a very small sample of people who have actually read and understood the book. I would instead compare yourself to a much larger sample who say they want to improve their data skills but never actually do anything about it. You\u2019ve done the hard work and now you\u2019ve got the theory to apply in practice that many don\u2019t! Well done!", "author": "GlueSniffingEnabler", "created_utc": "2024-06-09 06:40:23", "permalink": "/r/dataengineering/comments/1dbax05/how_long_did_it_take_you_to_read_database_system/l7s6iue/"}
{"body": "Pretty sure you shouldn\u2019t find it hard finding a UK job that pays more. I\u2019m remote, senior DE in the UK, get paid 10k more than you and that\u2019s still below market rate.", "author": "likes_rusty_spoons", "created_utc": "2024-06-09 06:38:36", "permalink": "/r/dataengineering/comments/1dbmc5i/looking_for_a_remote_job_as_senior_de/l7s6d1y/"}
{"body": "Thanks! I'm definitely proud of the project...I just wish I knew more about more modern tools that data engineers use. I feel so behind building everything with PowerShell, C# and SQL Server... Don't get me wrong, it works great, I just feel like I'm falling behind/missing out.", "author": "chadbaldwin", "created_utc": "2024-06-09 06:36:12", "permalink": "/r/dataengineering/comments/1d92cm0/discussion_i_built_a_tool_for_analyzing_sql/l7s65c6/"}
{"body": "We version control and utilize CI/CD for azure data factory. You just need to choose to do it.", "author": "droosif", "created_utc": "2024-06-09 06:20:06", "permalink": "/r/dataengineering/comments/1d9o0sv/how_much_coding_do_data_engineers_do/l7s4pl2/"}
{"body": "Yeah, OP spent more time writing this thread than writing those joins", "author": "UpstairsEye4793", "created_utc": "2024-06-09 06:12:24", "permalink": "/r/dataengineering/comments/1dafb24/sql_query_help/l7s40hf/"}
{"body": "provide more details", "author": "ab624", "created_utc": "2024-06-09 06:05:44", "permalink": "/r/dataengineering/comments/1dbe4c0/choosing_between_aws_glue_and_emr_serverless_for/l7s3ekp/"}
{"body": "US companies look for work.permit visas and generally won't accept to let their data operated outside continent. More over there are lots of highly skilled DEs within US", "author": "rajekum512", "created_utc": "2024-06-09 05:55:38", "permalink": "/r/dataengineering/comments/1dbmc5i/looking_for_a_remote_job_as_senior_de/l7s2h7q/"}
{"body": "all, I'm not part of the decision maker so i dont know the rationality and dont really have a say about it. Im more on the part of implementing it and maintaining it.\n\nWith the other folks that think we should not, can you share why or the problems you had why youre against it?", "author": "elijahlucas829", "created_utc": "2024-06-09 05:46:02", "permalink": "/r/dataengineering/comments/1db3a6m/redshift_implementation/l7s1jz6/"}
{"body": "They give you a really good interview guide (best I have ever seen), it's just lists/dicts/arrays.\u00a0", "author": "lastchancexi", "created_utc": "2024-06-09 05:34:13", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7s0f86/"}
{"body": "I would pick EMR server less since it's closer to pure play EMR, and you are not lost in the aws specific glue abstractions. While the outcome is same in both cases, EMR will be a generic solution and portable to multiple platforms.", "author": "abhi5025", "created_utc": "2024-06-09 05:12:34", "permalink": "/r/dataengineering/comments/1dbe4c0/choosing_between_aws_glue_and_emr_serverless_for/l7rycks/"}
{"body": "Just got involved in a project where we are using terraform 1.8.0 to provision aws components and also some snowflake. Migrated to dbt for the tables bc the terraform snowflake provider wasn\u2019t handling ddl changes well. But schemas and external stages are still in TF. What comes to your mind regarding this setup? What should i look into/research for long term?", "author": "Old-Evening9609", "created_utc": "2024-06-09 05:06:47", "permalink": "/r/dataengineering/comments/1dbh929/terraform_vs_opentofu/l7rxs36/"}
{"body": "Absolutely, don't do this!  You are adopting legacy tech. It is slow.  It is nowhere near as good as most of the other options.", "author": "discord-ian", "created_utc": "2024-06-09 05:02:31", "permalink": "/r/dataengineering/comments/1db3a6m/redshift_implementation/l7rxcqt/"}
{"body": "We have an ML model that can predict how long a query will take to run.", "author": "mirasume", "created_utc": "2024-06-09 04:26:25", "permalink": "/r/dataengineering/comments/1c5pcos/my_project_save_50_on_snowflake_in_15_minutes/l7rtln6/"}
{"body": "What kind of python tasks? Like pyspark or other?", "author": "gman1023", "created_utc": "2024-06-09 03:42:01", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7ropnc/"}
{"body": "for each row in the vulnerabiltiies table, there can be multiple groups since each host can be associated with multiple groups? if so, then you'd need to join vulnerabilities to host to group bridge table (many to many join) -- filtering for host-to-groups based on the row-valid columns -- and then aggregate afterwards.", "author": "yo_sup_dude", "created_utc": "2024-06-09 03:04:18", "permalink": "/r/dataengineering/comments/1darvjw/how_to_track_historical_aggregates_in_scd2_based/l7rk84m/"}
{"body": "I wouldn't compare them. Firestore is closer to Dynamo. I'd use BigTable more for very large scale time series and analytics use cases. Dynamo can power applications of all sizes. It's very likely being used for far more things where BigTable would be overkill.\n\nBigTable is also wide column oriented (think Cassandra or HBase) rather than key-value oriented like Dynamo or Firestore. I think AWS has a managed Cassandra option.", "author": "mailed", "created_utc": "2024-06-09 02:59:11", "permalink": "/r/dataengineering/comments/1d9hqoy/why_is_dynamodb_disproportionately_more_popular/l7rjjxa/"}
{"body": "Assuming they mean data structures and algorithms", "author": "Lower_Sun_7354", "created_utc": "2024-06-09 02:51:42", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7rilna/"}
{"body": "Wow that's really interesting, def not what I was expecting haha, thanks for the rec I'll absolutely take a look.", "author": "Ok-Carpet-2891", "created_utc": "2024-06-09 02:50:54", "permalink": "/r/dataengineering/comments/1dbe3j2/favorite_resources_related_to_project_management/l7rii3p/"}
{"body": "Thank you!", "author": "Few_Barber_8292", "created_utc": "2024-06-09 02:42:55", "permalink": "/r/dataengineering/comments/1dbh929/terraform_vs_opentofu/l7rhhln/"}
{"body": "DSA?", "author": "Uwwuwuwuwuwuwuwuw", "created_utc": "2024-06-09 02:31:17", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7rfzcd/"}
{"body": "Probably no need for Spark. Could probably just use Athena or even DuckDB for what you described.", "author": "pi-equals-three", "created_utc": "2024-06-09 02:30:36", "permalink": "/r/dataengineering/comments/1dbe4c0/choosing_between_aws_glue_and_emr_serverless_for/l7rfw5v/"}
{"body": "Dont", "author": "Doyale_royale", "created_utc": "2024-06-09 02:28:39", "permalink": "/r/dataengineering/comments/1db3a6m/redshift_implementation/l7rfn3v/"}
{"body": "I would read *The Phoenix Project* to get your appetite whet on this topic. It's a fiction book written by folks who literally wrote the book on DevOps. Certainly not the end all on the topic, but I think it will introduce you to lots of topics you may want to explore further.", "author": "Known-Huckleberry-55", "created_utc": "2024-06-09 02:16:50", "permalink": "/r/dataengineering/comments/1dbe3j2/favorite_resources_related_to_project_management/l7re3x3/"}
{"body": "Do you really need Spark? You can run your tasks using ECS tasks in AWS.", "author": "Proud-Walk9238", "created_utc": "2024-06-09 02:16:34", "permalink": "/r/dataengineering/comments/1dbe4c0/choosing_between_aws_glue_and_emr_serverless_for/l7re2nn/"}
{"body": "Thanks mate, I'll DM you", "author": "mailed", "created_utc": "2024-06-09 02:11:58", "permalink": "/r/dataengineering/comments/1d92m2s/what_are_some_rarely_explored_niches_in_data/l7rdgbi/"}
{"body": "What are you going to do about 01/02/2024? Is that Jan 2 or Feb 1? You'd need to detect across known data which is which for XX/ZZ/YYYY, such as only ZZ &gt;=12 so XX is months and format for that range is MM/DD/YYYY.\n\nIf this is known and its just you have \n\n2024-06-09T00:00:00Z  \nOr 2024-06-09 00:00:00.000-07:00  \n06/09/2024 00:00:00\n\nWith no month/day col ambiguity\n\nJust identify the format and timezone, and apply the correct transformations, whether SQL or Python, these are pretty standard so date helper functions exist and Python has some auto datetime formatters but can't speak to reliability", "author": "Monowakari", "created_utc": "2024-06-09 02:02:41", "permalink": "/r/dataengineering/comments/1dbfkmd/timestamp_column_has_multiple_formats/l7rc859/"}
{"body": "It\u2019s Columbus, OH\n\nI wish I asked for more, but I\u2019m also happy with what I have. Comparison is the thief of joy", "author": "BigSpartan84", "created_utc": "2024-06-09 02:02:38", "permalink": "/r/dataengineering/comments/1d5q7db/quarterly_salary_discussion_jun_2024/l7rc7us/"}
{"body": "EMR probably cheaper, glue prob easier to roll with", "author": "chrisrules895", "created_utc": "2024-06-09 01:47:47", "permalink": "/r/dataengineering/comments/1dbe4c0/choosing_between_aws_glue_and_emr_serverless_for/l7ra8jd/"}
{"body": "are they giving up on delta ?", "author": "Electrical-Ask847", "created_utc": "2024-06-09 01:28:22", "permalink": "/r/dataengineering/comments/1da910n/what_reasons_do_i_have_to_keep_any_data_in/l7r7o9m/"}
{"body": "Don\u2019t use it. \nSpec out cost of data center and buy your own on-prem machines. The cost saving is staggering after a few years if your typical workload requires high memory spec host.", "author": "Adorable-Employer244", "created_utc": "2024-06-09 01:24:19", "permalink": "/r/dataengineering/comments/1dae1uz/how_did_you_reduce_your_databricks_costs/l7r74ot/"}
{"body": "Everything at Meta is internal. As a DE you would be working on something similar to Airflow + Presto/Spark and probably a few other things. The only thing they care about is Python and SQL.\n\nSource: Ex-Meta DE", "author": "Ok-Muffin-8079", "created_utc": "2024-06-09 01:18:25", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7r6cwc/"}
{"body": "Okay thanks so much!", "author": "Acrobatic_Sample_552", "created_utc": "2024-06-09 01:12:15", "permalink": "/r/dataengineering/comments/1db4j6e/how_do_you_configure_a_data_lake_to_connect/l7r5jem/"}
{"body": "I'm reading the official DMBOK book (the one you can physically kill a human being with), also with help of GPT.", "author": "Lemonade-Candy-121", "created_utc": "2024-06-09 01:11:51", "permalink": "/r/dataengineering/comments/1daznz3/are_there_anyone_preparing_for_cdmp_dmbok/l7r5hiu/"}
{"body": "\n1. SR BI Engineer.\n2. 2 YOE after officially pivoting to data only roles. This is after 6-7 years of more jack of all trades roles in IT which included business analysis and reporting duties, along with some implementation projects.\n3. Ohio (100% remote).\n4. 140k base.\n5. 10% of base. $75k in equity vested over 4 years.\n6. SaaS (but on the business side).\n7. Snowflake, DBT, Python, AWS.\n\nI started this role very recently. I'll be staying here as long as I can. It's a great company with room for growth. Previous role I was at 118k, no bonuses or equity, but still 100% remote.", "author": "cream_pie_king", "created_utc": "2024-06-09 01:09:57", "permalink": "/r/dataengineering/comments/1d5q7db/quarterly_salary_discussion_jun_2024/l7r58fm/"}
{"body": "You can get Carfax and Autocheck for only $10. [click here](https://www.paypal.com/ncp/payment/LHCZAWAZXDG3G)", "author": "FewProfessional3823", "created_utc": "2024-06-09 01:07:16", "permalink": "/r/dataengineering/comments/18s5vnb/how_does_carfax_get_all_of_its_data/l7r4vtv/"}
{"body": "I mean, the DE stack is pretty much custom-built Airflow, but they don't care about any of that in the interview.", "author": "lastchancexi", "created_utc": "2024-06-09 01:06:01", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7r4pxy/"}
{"body": "That's a great point regarding \"catching off guard.\" Thanks for your feedback!", "author": "on_the_mark_data", "created_utc": "2024-06-09 00:57:14", "permalink": "/r/dataengineering/comments/1d9zyas/how_would_you_like_to_learn_about_data_quality/l7r3k7b/"}
{"body": "IBM bought hashi Corp. \u00a0I wouldn\u2019t say IBM has been great as of late, but they know how to steam a corporate ham. \u00a0\n\nI would focus on using 1.5.7 and then it works either way. My team is on a much earlier version of TFE and we moved to Scalr last fall. \u00a0\n\nWe are a large fortune 50 company and are leaning open tofu, but I wouldn\u2019t say it\u2019s a total lock just yet. \u00a0", "author": "vandelay82", "created_utc": "2024-06-09 00:45:43", "permalink": "/r/dataengineering/comments/1dbh929/terraform_vs_opentofu/l7r215w/"}
{"body": "I'm going to take this opportunity to hate on dashboards, even if I'm alone in my hatred.\n\n\nScrew Power BI, and screw tableau, and screw Looker, and screw the rest of them too.\n\n\nI can understand that when you need to provide a place for other people to self-serve data, they can be a great hub. But when I need to glance over 40 metrics each week to see how they're ticking along, I find it much easier to have all 40 graphs laid out in Excel without the need for me to apply any filters or change views.", "author": "ColdStorage256", "created_utc": "2024-06-09 00:28:40", "permalink": "/r/dataengineering/comments/1d9s2qe/what_are_everyones_hot_takes_with_some_of_the/l7qzqy6/"}
{"body": "You shouldn't need AI to tell you how many widgets you sold last year but it could be very useful in helping you answer \"How many widgets will we sell to Germany next year?\"", "author": "ColdStorage256", "created_utc": "2024-06-09 00:21:35", "permalink": "/r/dataengineering/comments/1d9s2qe/what_are_everyones_hot_takes_with_some_of_the/l7qysl0/"}
{"body": "5 week", "author": "CrayonUpMyNose", "created_utc": "2024-06-09 00:20:25", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7qymyt/"}
{"body": "FAANG does not care about tools. Just DSA + system design + behavioral.", "author": "boboshoes", "created_utc": "2024-06-09 00:06:21", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7qwqgp/"}
{"body": "As a Manager, Fabric is the primary choice because execs want to listen to a salesman vs. literally everyone that manages or uses the platform.", "author": "No_Cover_Undercover", "created_utc": "2024-06-08 23:50:23", "permalink": "/r/dataengineering/comments/1dan6w0/are_databricks_really_going_after_snowflake_or_is/l7qul7h/"}
{"body": "Meta\u2019s tech stack is entirely internally developed tools. They have a 2 week onboarding bootcamp where they teach them to you.", "author": "its_PlZZA_time", "created_utc": "2024-06-08 23:41:33", "permalink": "/r/dataengineering/comments/1dbf4xg/meta_tech_stack/l7qtdhi/"}
{"body": "Get really conversant in AWS/Azure/GCP (whatever your company uses) and how to build production quality apps. Learn Docker forwards and back. Understand messaging systems really well. Ever read DDIA? If not do.", "author": "AlgoRhythmCO", "created_utc": "2024-06-08 23:16:50", "permalink": "/r/dataengineering/comments/1dafrbc/what_should_i_focus_on_if_i_want_to_eventually/l7qpxz5/"}
{"body": "Check out [Zooniverse](https://www.zooniverse.org/). I haven't tried it myself but it seems that there are research projects which probably could do with some technical advice.", "author": "NASAOfficialAccount", "created_utc": "2024-06-08 23:13:11", "permalink": "/r/dataengineering/comments/1dbe8p1/i_would_like_collaborate_on_data_projects/l7qpftx/"}
{"body": "Glue itself runs on EMR. You have more features around it like crawler, dq stuff and its integration with Athena etc\n\nThink from use cases that will drive your needs", "author": "ZeroMomentum", "created_utc": "2024-06-08 23:06:30", "permalink": "/r/dataengineering/comments/1dbe4c0/choosing_between_aws_glue_and_emr_serverless_for/l7qoild/"}
{"body": "well, we are just pretty young in using adf and everyone developing the pipeline doesn't always follow best practices :)", "author": "spacehamba", "created_utc": "2024-06-08 22:51:24", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7qmer9/"}
{"body": "imply druid", "author": "Electrical-Ask847", "created_utc": "2024-06-08 22:44:21", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7qlf8d/"}
{"body": "Try open source", "author": "CrowdGoesWildWoooo", "created_utc": "2024-06-08 22:31:45", "permalink": "/r/dataengineering/comments/1dbe8p1/i_would_like_collaborate_on_data_projects/l7qjnnj/"}
{"body": "Can you give a review of your first 3 weeks?", "author": "Spicy_Latina29", "created_utc": "2024-06-08 22:18:44", "permalink": "/r/dataengineering/comments/194304v/reviews_on_data_engineer_academy/l7qhu99/"}
{"body": "You can find our open-source project showcase here: https://dataengineering.wiki/Community/Projects\n\nIf you would like your project to be featured, submit it here: https://airtable.com/appDgaRSGl09yvjFj/pagmImKixEISPcGQz/form\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "author": "AutoModerator", "created_utc": "2024-06-08 22:13:41", "permalink": "/r/dataengineering/comments/1dbe8p1/i_would_like_collaborate_on_data_projects/l7qh4p4/"}
{"body": "&gt; Even though they show power bi and DF as inside the fabric boundary on heir diagrams.\n\nFabric has re-implementations of all of these items. Imagine they've forked the repo and integrated them into some all-inclusive package. The ADF you get in Azure is not the same as the one you get in Fabric, though it will be very similar.", "author": "azirale", "created_utc": "2024-06-08 22:08:09", "permalink": "/r/dataengineering/comments/1dan6w0/are_databricks_really_going_after_snowflake_or_is/l7qgccz/"}
{"body": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "author": "AutoModerator", "created_utc": "2024-06-08 22:06:58", "permalink": "/r/dataengineering/comments/1dbe3j2/favorite_resources_related_to_project_management/l7qg6bx/"}
{"body": "There may be better ways to do this:\n\n   * Leverage the source system's transaction log: better for low-latency, the only solution if you need transaction-level changes, better for reducing impact to the source system, but worse for complexity: you really want a pre-built solution to support this.\n   * Source system publishes data that you then pick up:  better for creating a managed interface between two systems.\n\nRegarding the solution you've got a few things to consider:\n\n   * Timestamps on your upstream system may not be extremely precise: they may get assigned well before the transaction is actually committed.  The implication of this is that those rows may only become visible to your queries 3 seconds or 3 minutes AFTER the timestamp!  For this reason it's usually best to include some concept of a \"grace period\".  For example, modify your range to start at the end of the last query and end at the current time minus 3 to 5 minutes.\n   * I find that it's helpful to store the data I extract this was as part of my raw archive.  To make this most useful I have timestamps on each file and I ensure that no file spans a day or hour boundary.  So, it's easy to examine the data for any period of time - and possibly reprocess or reload it.\n   * Use an audit or control table to track what you've queried, how many rows returned, what the exact start/stop times are, what the actual process times are, and what the return codes are.  This can eliminate queries against the source system and give you easy reporting on how your system has been working.\n   * Consider building a tiny little reconciliation system:  something that counts rows after a period has been completed on both source &amp; destination and loads them into a table.  Then once there you can compare those rows to ensure they're identical - and you're not missing any entries.  Missing entries could be due to reprocessing by the source system, maybe their timestamps are applied at the app level and not very reliable, etc, etc.", "author": "kenfar", "created_utc": "2024-06-08 22:01:55", "permalink": "/r/dataengineering/comments/1db9bz8/best_practice_for_keeping_related_incremental/l7qfg1r/"}
{"body": "CrateDB seems like a good fit", "author": "surister", "created_utc": "2024-06-08 21:31:44", "permalink": "/r/dataengineering/comments/1daxd20/need_inputs_on_choosing_right_nosql_database_open/l7qb198/"}
{"body": "If you read the entire book in 3-4 months and actually understood all of the concepts in the book (note: most people who read the book don\u2019t really dive too deep), then yeah that\u2019s pretty impressive imo. Databases are an important topic and that book covers database at a much broader and deeper level than 90+% of devs need to know", "author": "yo_sup_dude", "created_utc": "2024-06-08 20:59:34", "permalink": "/r/dataengineering/comments/1dbax05/how_long_did_it_take_you_to_read_database_system/l7q65cu/"}
{"body": "I think the cloud strategy is better to base on your company's business requirement with certain stage and size. I can see many small to medium startups starting with managed data services provided by cloud vendors, such as GCP, AWS or Azure and I think all of them should be OK. But eventually, when a startup becomes a large company or acquired by a bigger company, then multi-cloud strategy comes after as you and your company would prefer to adopt a vendor neutral architecture rather than vendor-lockin. \n\nFor career prospective, yes, the experience with data infrastructure on multi-cloud is better than bet on any cloud only.", "author": "Brief_Waltz_6455", "created_utc": "2024-06-08 20:54:54", "permalink": "/r/dataengineering/comments/1dayxg5/what_cloud_strategy_we_should_start/l7q5fx8/"}
{"body": "A 4th table to control, especially if you have a common PK.", "author": "SirGreybush", "created_utc": "2024-06-08 20:54:40", "permalink": "/r/dataengineering/comments/1db9bz8/best_practice_for_keeping_related_incremental/l7q5emt/"}
{"body": "This post was flagged as not being related enough to data engineering. In order to keep the quality and engagement high, we sometimes remove content that is unrelated or not relevant enough to data engineering.", "author": "dataengineering-ModTeam", "created_utc": "2024-06-08 20:53:21", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7q57hg/"}
{"body": "OP: BigQuery outperforms its competitors in most use cases, both in speed and cost. I will say that GCP managed airflow is very rigid and limited, specially since you are already deploying your own, I would deploy my own airflow on GCP managed k8s. Also I would leverage k8s operator more since you would be already using it.", "author": "ALostWanderer1", "created_utc": "2024-06-08 20:12:30", "permalink": "/r/dataengineering/comments/1dayxg5/what_cloud_strategy_we_should_start/l7pywme/"}
{"body": "GCP is a natural fit, but AWS Glue can also be a great alternative.", "author": "Lost_Investigator297", "created_utc": "2024-06-08 20:10:38", "permalink": "/r/dataengineering/comments/1dayxg5/what_cloud_strategy_we_should_start/l7pymcm/"}
{"body": "I'm also studying for CDMP! What resources are you using to prep?", "author": "Lost_Investigator297", "created_utc": "2024-06-08 20:10:25", "permalink": "/r/dataengineering/comments/1daznz3/are_there_anyone_preparing_for_cdmp_dmbok/l7pyl7g/"}
{"body": "Have you considered Apache Cassandra for high write throughput and scalability?", "author": "Lost_Investigator297", "created_utc": "2024-06-08 20:10:11", "permalink": "/r/dataengineering/comments/1davs19/dynamodb_or_other_options/l7pyjtv/"}
{"body": "Have you considered OrientDB? It's scalable, NoSQL, and has sync capabilities.", "author": "Lost_Investigator297", "created_utc": "2024-06-08 20:09:29", "permalink": "/r/dataengineering/comments/1daxd20/need_inputs_on_choosing_right_nosql_database_open/l7pyg26/"}
{"body": "Check out MongoDB's success, it's a great example of OSS + premium cloud model.", "author": "Lost_Investigator297", "created_utc": "2024-06-08 20:09:15", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7pyeqo/"}
{"body": "Everyone has its own pace, nonetheless I believe that going through such hard and long piece of knowledge, with properly understanding it in timespan you mentioned is pretty good, and the best thing is you had a discipline to go through it :)\n\nKeep it up!", "author": "Natural-Tune-2141", "created_utc": "2024-06-08 20:07:24", "permalink": "/r/dataengineering/comments/1dbax05/how_long_did_it_take_you_to_read_database_system/l7py4lz/"}
{"body": "To be honest, I am an old computer engineer. My focus was on building home computers to sell, repair and upgrade. But that has no money in it then I moved to operations in few companies. And As any computer tech would tell you, we are required to do so much with so little and in the end we get dumped on when idiots break their computers. So, now I\u2019m thinking maybe get into servers hardware tech and go from there.", "author": "makzero", "created_utc": "2024-06-08 20:02:06", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pxblu/"}
{"body": "Oh okay, I didn\u2019t know that was an option", "author": "makzero", "created_utc": "2024-06-08 19:59:34", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pwxi0/"}
{"body": "Virtual private server", "author": "WiscoSippi", "created_utc": "2024-06-08 19:57:32", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pwm6a/"}
{"body": "Virtual private server.  Just a VM in the cloud that you can do whatever you want with.  You won't be able to to physical hardware stuff, but it's way cheaper than buying a server &amp; paying the electric bill to run it.", "author": "jaredrileysmith", "created_utc": "2024-06-08 19:57:32", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pwm5h/"}
{"body": "How did you manage that?", "author": "SuitCool", "created_utc": "2024-06-08 19:48:42", "permalink": "/r/dataengineering/comments/1daj3n0/what_do_you_predict_with_be_announced_at_the/l7pv8y5/"}
{"body": "You are right, that\u2019s a Great point.", "author": "makzero", "created_utc": "2024-06-08 19:42:01", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pu7pw/"}
{"body": "\ud83d\ude2f I didn\u2019t know any of that existed. Thank you", "author": "makzero", "created_utc": "2024-06-08 19:41:13", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pu3fg/"}
{"body": "What\u2019s a vps?", "author": "makzero", "created_utc": "2024-06-08 19:40:09", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7ptxlt/"}
{"body": "Oh okay. That\u2019s great information thank you", "author": "makzero", "created_utc": "2024-06-08 19:39:35", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7ptuho/"}
{"body": "Definitely overkill for just tinkering. And you will figure\u00a0that out as soon as you set eyes on it or buy it and plug it in.\u00a0\n\n\nI suggest you identify the types of tinkering or projects you'd like to work on and look into how to make that work with your existing hardware. For example, you can spin up Developer/Enterprise SQL Server instances on docker in just a few minutes. The same goes for many new platforms that are out there today. Python virtual environments or Conda are great too.\u00a0\n\n\nAll of my old PC gaming hardware gets a 2nd life as server hardware, including old graphics cards. Those are useful for\u00a0hardware acceleration to transcode videos, or loading quantized LLMs for example.", "author": "benchwrmr22", "created_utc": "2024-06-08 19:38:02", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7ptlw9/"}
{"body": "Will it work? Yes. But most likely it will be slower. Bigtable probably sorts by key, which makes lookups fast. This is not true for OS filesystem.", "author": "hadoopfromscratch", "created_utc": "2024-06-08 19:32:58", "permalink": "/r/dataengineering/comments/1db88q6/key_value_conundrum/l7pstyx/"}
{"body": "The most important factor to consider is the power usage. If you intend to use this server for file backups, it will need to run 24/7, which can significantly increase your expenses. If you choose not to run the server continuously, it could be a worthwhile investment. Objectively, it may not be the best purchase, but it depends on your specific needs and or goals. you can use tools online to estimate power consumption e.g. [https://www.rapidtables.com/calc/electric/energy-cost-calculator.html](https://www.rapidtables.com/calc/electric/energy-cost-calculator.html)", "author": "whiteboreded", "created_utc": "2024-06-08 19:29:06", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7ps8pk/"}
{"body": "NO!  The CPU architecture is 14 years old!\n\nFor a few hundred more, you can get a first gen Scalable Xeon that will run circles around these systems.  \n\nI'd also recommend getting a pedestal server instead of a rack mount.  Your ears will thank you.\n\nOr as others have pointed out, get a VPS and a backup service like Backblaze.  I think they offer 10GB of blob storage for free.", "author": "Visual_Cabinet_3718", "created_utc": "2024-06-08 19:25:42", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7prq4s/"}
{"body": "We use a metadata table with seperate schema and permissions for adf parameters. \nWe store the run id in a table for debugging the previous runs and take the output from the metadata table via stored procedures.", "author": "EvenChilli2341", "created_utc": "2024-06-08 19:11:39", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7ppl16/"}
{"body": "Use the IMDB db, pop culture is relateble", "author": "Drunk_redditor650", "created_utc": "2024-06-08 19:08:44", "permalink": "/r/dataengineering/comments/1dal3z9/how_did_you_find_ideas_for_portfoliopersonal/l7pp5b2/"}
{"body": "We have Azure Cosmos DB storing semi-structured data. We use azure functions to connect to Cosmos db in the pipeline, pull the data from cosmos db, store in in a variable and then use that variable to provide dynamic values to parameters. \n\nThis is one of the examples, but this helps us reduce no. of pipelines significantly. For one pipeline, we could do multiple runs for different values. You can use other database as well.   \n  \nI learnt from this youtuber, they demonstrated how to dynamically map schema, you can use same concept to dynamically pass values to parameters: [https://youtu.be/b27gmOufge4](https://youtu.be/b27gmOufge4)   \n  \n(as this is for mapping, adf understands json, the person needs to ultimately pass value in json format, for parameter it shouldn't be an issue)", "author": "Mediocre_Plum_7573", "created_utc": "2024-06-08 18:57:40", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7pnfuh/"}
{"body": "Why do you need so many parameters lol.", "author": "Demistr", "created_utc": "2024-06-08 18:55:46", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7pn5bw/"}
{"body": "And be claustrophobic in his room", "author": "Jealous-Weekend4674", "created_utc": "2024-06-08 18:55:40", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pn4pw/"}
{"body": "you think they'd open source it given the Polaris news?", "author": "ruffruff23", "created_utc": "2024-06-08 18:53:16", "permalink": "/r/dataengineering/comments/1daj3n0/what_do_you_predict_with_be_announced_at_the/l7pmras/"}
{"body": "and heat his room.", "author": "DrSFalken", "created_utc": "2024-06-08 18:36:23", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pk4dh/"}
{"body": "Just rent a VPS for like $10 a month", "author": "jaredrileysmith", "created_utc": "2024-06-08 18:34:53", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pjw1y/"}
{"body": "they serve different needs. I would put Dagster in the orchestration camp and dbt in the transformation one . you also get docs, lineage, and data quality.  If using polars you wouldnt get all that.", "author": "Hot_Map_7868", "created_utc": "2024-06-08 18:29:44", "permalink": "/r/dataengineering/comments/1d7fhyz/why_use_dbt_if_i_have_dagster/l7pj3jm/"}
{"body": "Increase his electricity bill.", "author": "cl326", "created_utc": "2024-06-08 18:19:52", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7phkt2/"}
{"body": "Snowflake to Stage is not that different than Snowflake -&gt; something in the middle -&gt; S3. I think this is potentially less secure since you have the middle step.  If you have the S3 keys, you can set up the stage in Snowflake and unload the data that way.  I think the aversion to not using a stage is probably due to a lack of understanding.", "author": "Hot_Map_7868", "created_utc": "2024-06-08 18:18:43", "permalink": "/r/dataengineering/comments/1d49quu/help_with_big_data/l7phef0/"}
{"body": "I think Mongo\u2019s device sync only works with their cloud offering", "author": "chickenparmesean", "created_utc": "2024-06-08 18:17:51", "permalink": "/r/dataengineering/comments/1daxd20/need_inputs_on_choosing_right_nosql_database_open/l7ph9lz/"}
{"body": "I've been working in the field for many years, most of the time you try to understand the systems data and dig through it, sometimes you do python, java use things like dbt, talend, airflow and OfCourse a lot of SQL", "author": "Zestyclose_Winter363", "created_utc": "2024-06-08 18:16:04", "permalink": "/r/dataengineering/comments/1d9o0sv/how_much_coding_do_data_engineers_do/l7pgzrn/"}
{"body": "I dont think you need a docker image per pipeline. If using airflow, you can have an image that has your dependencies and then sync the project to airflow via S3 / git sync.", "author": "Hot_Map_7868", "created_utc": "2024-06-08 18:14:20", "permalink": "/r/dataengineering/comments/1d05l60/dockerizing_pipelines/l7pgpt6/"}
{"body": "It is common to need multiple tools because different tools are better/more cost effective for different things. \n\nCheck out Fivetran, Airbyte, dlthub, and Striim. It is common to also need an orchestrator to trigger the EL and then call the transformation tool like dbt once data has been loaded. For this check out Dagster, Astronomer, or Datacoves.", "author": "Hot_Map_7868", "created_utc": "2024-06-08 18:12:25", "permalink": "/r/dataengineering/comments/1d1yrfa/own_data_ingestion_pipeline_or_commercial/l7pgey3/"}
{"body": "Fabric / Azure stuff is a joke. Databricks is also largely a joke, but they let you keep it simple and use them as a glorified VM runner for \u201creal\u201d code. And running your own Lakehouse-ish stack by keeping data on Blob.", "author": "kebabmybob", "created_utc": "2024-06-08 18:09:47", "permalink": "/r/dataengineering/comments/1dan6w0/are_databricks_really_going_after_snowflake_or_is/l7pg08h/"}
{"body": "If they have the same schema, you can query them in ensemble with DuckDB to write out to, say, parquet files. For incremental batches and if you have access to it, Databricks is a good tool.", "author": "memeorology", "created_utc": "2024-06-08 18:06:09", "permalink": "/r/dataengineering/comments/1dahu1p/move_large_number_of_small_csv_files_to_archive/l7pffk4/"}
{"body": "Dynamic pipelines and stored instructions in SQL, you call them via Script activity.", "author": "vermillion-23", "created_utc": "2024-06-08 18:06:03", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7pfezs/"}
{"body": "Ask a lot of questions (without worrying about looking like a dummy), and honestly - the question 80% of the time should be, \"Why?\"\n\nSTRONG caveat here: I agree with your friend, not because of burnout - but you don't want to be the new hire that says \\[insert your most stereotypical nerd voice impression here\\], \"Well, on this URL of the dbt documentation in page 2.3 it says \\[X\\], and I can't believe you wouldn't do \\[Y\\] since I'm now a dbt genius, ...\"\n\nBetter questions sound more like, \"It looks like you're using transformations to create a medallion system - but to be honest, I'm not quite getting the business requirements. Can we take a few minutes where you can walk me through what the heck the business folks are looking for as their ultimate outcome?\"", "author": "creepystepdad72", "created_utc": "2024-06-08 17:58:03", "permalink": "/r/dataengineering/comments/1dad3e7/accepted_as_a_jr_data_engineer/l7pe5f2/"}
{"body": "::sigh:: \u2026 ::zip::", "author": "IdealState", "created_utc": "2024-06-08 17:53:02", "permalink": "/r/dataengineering/comments/1daj3n0/what_do_you_predict_with_be_announced_at_the/l7pdcym/"}
{"body": "This is not a good use case of dynamo. Does it make sense to sort phone numbers? Dynamo works great when your sort keys are actually something that benefits from being sorted", "author": "ReporterNervous6822", "created_utc": "2024-06-08 17:50:28", "permalink": "/r/dataengineering/comments/1davs19/dynamodb_or_other_options/l7pcy97/"}
{"body": "That's how I use it, but I'm looking to switch to a better orchestrator.\n\nIt's basically good enough until it isn't.", "author": "babygrenade", "created_utc": "2024-06-08 17:49:48", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7pcufm/"}
{"body": "Nothing wrong with adf if you just use it as an orchestrator and SQL procs for transformation", "author": "kiwi_bob_1234", "created_utc": "2024-06-08 17:46:50", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7pcdj8/"}
{"body": "I was looking for a server setup that would be as new as possible and as cheap as possible to do experiments on with to work as a server tech while also serve as home data backup system for all of our devices. I know it\u2019s a long order lol but one can dream, am I right? Lol", "author": "makzero", "created_utc": "2024-06-08 17:44:18", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pbz7i/"}
{"body": "Careful.\n\nContext from a leadership standpoint: I absolutely want you to level-up and learn new skills, but the caveat is said things need to be for the benefit of the business. It's not cool if you're making choices on production infrastructure based on things you feel like learning or what looks good on a resume.\n\nThe upside here is if you're focused on the business outcomes, there's about a million technical directions you can take/things you can learn - but your decision needs to be 100% based on it being the right technical choice to make.", "author": "creepystepdad72", "created_utc": "2024-06-08 17:44:07", "permalink": "/r/dataengineering/comments/1dayxg5/what_cloud_strategy_we_should_start/l7pby3m/"}
{"body": "Learn Iceberg. You will thank me later.", "author": "Historical-Papaya-83", "created_utc": "2024-06-08 17:41:01", "permalink": "/r/dataengineering/comments/1dad3e7/accepted_as_a_jr_data_engineer/l7pbg2a/"}
{"body": "Keep those parameters in line or they'll go off the grid and cause chaos in your ADF pipeline!", "author": "bubbleinglenook", "created_utc": "2024-06-08 17:39:27", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7pb6xm/"}
{"body": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "author": "AutoModerator", "created_utc": "2024-06-08 17:36:26", "permalink": "/r/dataengineering/comments/1db88q6/key_value_conundrum/l7papi8/"}
{"body": "What are you going to do with it?", "author": "WayyyCleverer", "created_utc": "2024-06-08 17:35:39", "permalink": "/r/dataengineering/comments/1db7v1u/is_this_a_good_deal_listed_in_my_local_facebook/l7pakxu/"}
{"body": "Where I am they use Talend to connect to various APIs. Talend is the scheduler and the orchestrator. \n\nI do not wish this solution on anyone. \n\nMe, personally, would be Python, BitBucket, Jenkins and AirFlow. I like code.  Where I work they want No-Code. \n\nUse a tool to get all the json data files to the Datalake, which are API captures based on events or datetime. \n\nOnce there, it\u2019s just making sql views and managing files. Next step is aggregation, a whole other topic.", "author": "SirGreybush", "created_utc": "2024-06-08 17:32:15", "permalink": "/r/dataengineering/comments/1db4j6e/how_do_you_configure_a_data_lake_to_connect/l7pa1ml/"}
{"body": "Do you have any context on what the company/application does?\n\nI'm in the camp that the choice to go NoSQL should be made almost exclusively based on the business requirements - ie. \"Does it make logical sense based on what we do to adopt that type of structure?\"\n\nIf you don't need a very unique/fluid data structure for each user/record/whatever, you're signing yourself up for a world of pain.\n\nI've lived through this, where we'd acquired a company that used Mongo (in fairness, for legitimate reasons in the early days), who pivoted a year or two later but never switched over the back-end DB methodology (where the new business model by the time we acquired them was the stock example in textbooks on relational databases). \n\nI'm not exaggerating when I say it took about 5x longer than it should have to make any kind of code changes because the data structure was completely mis-fit from the business model.", "author": "creepystepdad72", "created_utc": "2024-06-08 17:30:53", "permalink": "/r/dataengineering/comments/1daxd20/need_inputs_on_choosing_right_nosql_database_open/l7p9tqi/"}
{"body": "Whatever solution you use you should at least add an \"created_at\" and a true unique  pk field", "author": "sazed33", "created_utc": "2024-06-08 17:30:05", "permalink": "/r/dataengineering/comments/1davs19/dynamodb_or_other_options/l7p9p7r/"}
{"body": "Three things. First, this is not _data engineering_ which is a specialization that is concerned on moving around and storing large amounts of data and all the infrastructure and code related to these tasks.\n\nSecond... Are you seriously hoping we would be able to help you with a problem stated in Hebrew?\n\nAnd third, maybe you need a dedicated sub for these things. Not here. I don't know of any unfortunately. Maybe r/csmajors? But I don't know.", "author": "FlowOfAir", "created_utc": "2024-06-08 17:28:10", "permalink": "/r/dataengineering/comments/1db2fg8/data_flow_diagram_using_mermaid/l7p9e8p/"}
{"body": "Definitely the second option. My experience with data quality has always been \"yeah this is good, data pipelines are working and users are happy\". Then some shit happens upstream and all hell breaks loose. You have to duplicate the catching off guard part imo for the most authentic experience.", "author": "whiskeybandit", "created_utc": "2024-06-08 17:25:33", "permalink": "/r/dataengineering/comments/1d9zyas/how_would_you_like_to_learn_about_data_quality/l7p8zb0/"}
{"body": "Can I ask you where you did your volunteering? Or on what pages can you search for one?", "author": "BestWhishes", "created_utc": "2024-06-08 17:22:38", "permalink": "/r/dataengineering/comments/1dad3e7/accepted_as_a_jr_data_engineer/l7p8il7/"}
{"body": "It wasn't what you said, just how you said it.", "author": "isleepbad", "created_utc": "2024-06-08 17:15:43", "permalink": "/r/dataengineering/comments/1dae1uz/how_did_you_reduce_your_databricks_costs/l7p7exm/"}
{"body": "You\u2019ll never have to optimize a table or worry too much about the compute creating or updating that table. Processes like CDC are much easier with the apply changes into, you don\u2019t have to worry about calling the correct table version.", "author": "Doyale_royale", "created_utc": "2024-06-08 17:13:48", "permalink": "/r/dataengineering/comments/1daj3n0/what_do_you_predict_with_be_announced_at_the/l7p740k/"}
{"body": "Thanks I'll look into it.  Do you mind sharing how much data you are streaming from Oracle and what your costs look like?  Also, confluent is data streaming right?  Our use case is analytics, so not sure we need that level of data latency.", "author": "joemerchant2021", "created_utc": "2024-06-08 17:13:36", "permalink": "/r/dataengineering/comments/1daeehn/incremental_ingestion_of_an_oracle_data_source/l7p72xu/"}
{"body": "Even though some may not like ADF, and through your obvious passive-aggressive comment you made clear that you are on of those people, your comment doesn't help in any way at all. You could've just written nothing and it would've been more helpful.", "author": "xSyndicate58", "created_utc": "2024-06-08 17:07:00", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7p61gy/"}
{"body": "Get off of ADF", "author": "DRUKSTOP", "created_utc": "2024-06-08 17:04:17", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7p5m0h/"}
{"body": "This is solid in Ohio with 2YOE. Keep it up!", "author": "goatcroissant", "created_utc": "2024-06-08 17:03:30", "permalink": "/r/dataengineering/comments/1d5q7db/quarterly_salary_discussion_jun_2024/l7p5hpe/"}
{"body": "People are just entering into the database linked above", "author": "goatcroissant", "created_utc": "2024-06-08 16:56:39", "permalink": "/r/dataengineering/comments/1d5q7db/quarterly_salary_discussion_jun_2024/l7p4f03/"}
{"body": "That\u2019s just intelligence\u2026 for some reason it\u2019s more valuable if we fake it\u2026 \ud83d\ude43 maybe it\u2019d make more sense to me if I had an mba.", "author": "Repulsive_Lychee_106", "created_utc": "2024-06-08 16:55:58", "permalink": "/r/dataengineering/comments/1d9s2qe/what_are_everyones_hot_takes_with_some_of_the/l7p4b4x/"}
{"body": "Fundamentals of Data Engineering is a great book for explaining the data engineering lifecycle, but that's not what I'm asking for. \n\nI've recently started to read DDIA. I'm unsure if this is what I'm looking for either.\n\nI'm looking for a \"practitioner/execution\" guide, i.e., project/resource planning that also explains inflection points and the criteria I should use to make decisions at those points.", "author": "Cultured_dude", "created_utc": "2024-06-08 16:47:25", "permalink": "/r/dataengineering/comments/1danhvu/what_bookswebsites_should_i_read_to_help_me/l7p2ypy/"}
{"body": "The real winners in AI are data engineers. most just don\u2019t know it yet", "author": "chrisgarzon19", "created_utc": "2024-06-08 16:46:09", "permalink": "/r/dataengineering/comments/1d9s2qe/what_are_everyones_hot_takes_with_some_of_the/l7p2rjm/"}
{"body": "Snowflake", "author": "Cultured_dude", "created_utc": "2024-06-08 16:42:13", "permalink": "/r/dataengineering/comments/1danhvu/what_bookswebsites_should_i_read_to_help_me/l7p25g0/"}
{"body": "I firmly believe your default should be Postgres, And then the task is finding any good reasons why that won\u2019t work and why you should use a more bespoke solution.", "author": "AlgoRhythmCO", "created_utc": "2024-06-08 16:40:20", "permalink": "/r/dataengineering/comments/1daxd20/need_inputs_on_choosing_right_nosql_database_open/l7p1uvr/"}
{"body": "Fabric is stillborn\u00a0", "author": "josephkambourakis", "created_utc": "2024-06-08 16:37:06", "permalink": "/r/dataengineering/comments/1dan6w0/are_databricks_really_going_after_snowflake_or_is/l7p1cpo/"}
{"body": "Synapse was dead on arrival\u00a0", "author": "josephkambourakis", "created_utc": "2024-06-08 16:36:51", "permalink": "/r/dataengineering/comments/1dan6w0/are_databricks_really_going_after_snowflake_or_is/l7p1bay/"}
{"body": "I will be messaging you in 7 days on [**2024-06-15 16:35:40 UTC**](http://www.wolframalpha.com/input/?i=2024-06-15%2016:35:40%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7p14g7/?context=3)\n\n[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fdataengineering%2Fcomments%2F1db5ke6%2Fhow_to_efficiently_manage_parameters_in_adf%2Fl7p14g7%2F%5D%0A%0ARemindMe%21%202024-06-15%2016%3A35%3A40%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%201db5ke6)\n\n*****\n\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|\n|-|-|-|-|", "author": "RemindMeBot", "created_utc": "2024-06-08 16:36:28", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7p195j/"}
{"body": "!RemindMe 7 days", "author": "Commercial-Ask971", "created_utc": "2024-06-08 16:35:40", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7p14g7/"}
{"body": "Company wants flexibility in terms of schema definition, that's why", "author": "1aumron", "created_utc": "2024-06-08 16:35:36", "permalink": "/r/dataengineering/comments/1daxd20/need_inputs_on_choosing_right_nosql_database_open/l7p1436/"}
{"body": "SAP SLT and HVR (now owned by Fivetran) are probably your only options if you have an old version of SAP without S/4 on top. (And don\u2019t want to hire a team of half baked SAP devs to write something custom for you). Fivetran/HVR have a couple of different connectors available, one of them might suit depending on your version, license, etc. What SAP versions are we talking? on HANA db?\n\nWe use SLT with the ADF connector for snowflake ingestion, and I know other companies are using the same set up for Databricks. SLT is a bit shit and very expensive to be honest, we only use it because our license doesn\u2019t allow us to use HVR because it uses direct log access for replication. Though apparently their new connectors don\u2019t need that.", "author": "kyleekol", "created_utc": "2024-06-08 16:35:03", "permalink": "/r/dataengineering/comments/1dav7hv/discussion_idea_brainstorming_for_sap_ingestion/l7p1134/"}
{"body": "Thanks for the idea, I\u2019ll check that out. \n\nYes we also try to optimize at job level by avoiding unnecessary shuffles, optimizing file sizes, pushing filters as soon as we can, and just making sure that, overall, cluster is used efficiently (looking at cpu/ram usage, idle time) We\u2019ve already optimized several jobs and divided their cost by 2 or 3 but I think that if we want to go deeper, we\u2019d have to really deep dive into technical details and it\u2019s not worth the time it would take given the human cost, especially given the savings we already did. \n\nI was more looking at \u00ab\u00a0simple solutions\u00a0\u00bb that we might have overlooked. For example when we switched from using i3 machines (recommended by DB) to fleet machines, we noticed big savings, and it was a simple modification in cluster configuration. Mind you it did not work for all jobs, but most of them.", "author": "Cyliad", "created_utc": "2024-06-08 16:28:22", "permalink": "/r/dataengineering/comments/1dae1uz/how_did_you_reduce_your_databricks_costs/l7ozzk1/"}
{"body": "idk im from on-prem gang. In general upsert is costly operation.   \nRedshift: considering redshift is built on top of postgres it should be slow and requires implementation. idk about state of support for \"merge\" or \"on conflict\" from pg. But it is only a theory, maybe it is good somehow - no real experience.\n\n  \nClickhouse is designed exactly for this case: upsert+aggregation of huge long tables, so it works out of the box with right engine - thats why CH usually used as a secondary DB for streaming data and data marts.\n\nNo info on snowflake, so. And i have no idea about cloud pricing on any of this", "author": "kolya_zver", "created_utc": "2024-06-08 16:27:28", "permalink": "/r/dataengineering/comments/1davs19/dynamodb_or_other_options/l7ozum0/"}
{"body": "Confluent has an oracle CDC connector. Costs money and is enterprise. We have been using the SaaS version for over a year now.", "author": "jelmew", "created_utc": "2024-06-08 16:26:03", "permalink": "/r/dataengineering/comments/1daeehn/incremental_ingestion_of_an_oracle_data_source/l7ozmr2/"}
{"body": "Very good place to start. Knowing the benefits AND the limitations is key.", "author": "Budget_Sherbet", "created_utc": "2024-06-08 16:19:52", "permalink": "/r/dataengineering/comments/1db3a6m/redshift_implementation/l7oyoka/"}
{"body": "can you share me any reference links on this concept.", "author": "spacehamba", "created_utc": "2024-06-08 16:18:10", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7oyf5q/"}
{"body": "As long as you keep in mind what you're selling, yes.", "author": "andpassword", "created_utc": "2024-06-08 16:10:37", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7ox3q5/"}
{"body": "sure let me explore this option.", "author": "spacehamba", "created_utc": "2024-06-08 16:01:32", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7oviy4/"}
{"body": "It seems you have tried out most of the options available for optimization.\n\nThis is at infra level. Did you try to optimize at job level ?\n\nJob level analytics might help reduce cost substantially.\n\nIf you can check if there is over allocation of resources.\n\nOne of tools we use for Job level optimization is Qubole's open Source Spark Lens you can use something similar which can point out job level issues.", "author": "Fun-Expression9347", "created_utc": "2024-06-08 15:58:23", "permalink": "/r/dataengineering/comments/1dae1uz/how_did_you_reduce_your_databricks_costs/l7ov151/"}
{"body": "Is this the 256 parameter limit for each ARM template? If so, think you can have a master ARM template and have additional linked templates..", "author": "rectoid247", "created_utc": "2024-06-08 15:53:27", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7ou976/"}
{"body": "Why insist on NoSQL?", "author": "figshot", "created_utc": "2024-06-08 15:42:46", "permalink": "/r/dataengineering/comments/1daxd20/need_inputs_on_choosing_right_nosql_database_open/l7osl7l/"}
{"body": "Lambdas, with the right framework, should be easy to debug and run. They are usually code that can run in your computer. No especializado parallel processing shit. \n\nThe 15' TO should be more than enough, even on the biggest lambdas. And even so, if that were the case, you can definitely go for something without that, like AWS Batch or some Fargate process, or some K8S template. But that would mean you're already way past your small data requirements that you are projecting to us right now.", "author": "camelCaseGuy", "created_utc": "2024-06-08 15:41:31", "permalink": "/r/dataengineering/comments/1d0lch3/question_this_data_architecture/l7ose85/"}
{"body": "Offload the parameters to a table in a database, or figure out how to reduce your pipelines to more generic pieces that can be reused where they are needed.", "author": "the_naysayer", "created_utc": "2024-06-08 15:41:19", "permalink": "/r/dataengineering/comments/1db5ke6/how_to_efficiently_manage_parameters_in_adf/l7osd10/"}
{"body": "Thanks for the feedback. I\u2019ll keep this in mind. Delta Lake seems to be more fleshed out and established.", "author": "ConvenientAllotment", "created_utc": "2024-06-08 15:38:40", "permalink": "/r/dataengineering/comments/1dan6w0/are_databricks_really_going_after_snowflake_or_is/l7ory1y/"}
{"body": "Elasticsearch!", "author": "josejo9423", "created_utc": "2024-06-08 15:33:19", "permalink": "/r/dataengineering/comments/1daxd20/need_inputs_on_choosing_right_nosql_database_open/l7or41n/"}
{"body": "Awesome, yeah most of that I knew except the native metastore, that I did not know.\n\nFor spectators Dremio supports HMS/Glue/Nessie soon REST Catalog.\n\nIn case you are there the next events I am pretty sure I\u2019ll be at with be Apache Community Over Code in Denver and Reinvent.", "author": "AMDataLake", "created_utc": "2024-06-08 15:28:47", "permalink": "/r/dataengineering/comments/1da910n/what_reasons_do_i_have_to_keep_any_data_in/l7oqeme/"}
{"body": "And to not leave Hudi out of the conversation, Trino (and Starburst) currently only have query support. No DDL or DML.", "author": "lester-martin", "created_utc": "2024-06-08 15:25:39", "permalink": "/r/dataengineering/comments/1da910n/what_reasons_do_i_have_to_keep_any_data_in/l7opwyp/"}
{"body": "Hey, Alex. I did make it to your session at Data Summit in NYC (I noticed you didn\u2019t come to any of mine; haha) and I saw how busy you were at the Dremio booth. Was hoping to introduce myself then, but we will surely cross paths again. Check out the Trino Iceberg connector docs at https://trino.io/docs/current/connector/iceberg.html which shows it can use Nessie, REST, JDBC, Glue, Snowflake, or HMS for a catalog. Have 3 different catalogs? Just configure 3 different setups and see them all (and you know the drill, join across them all). \n\nFor Delta Lake we need access to an HMS instance, glue, or Unity. \n\nAnd then yes, Starburst has its own (currently proprietary) metastore as another option.", "author": "lester-martin", "created_utc": "2024-06-08 15:23:19", "permalink": "/r/dataengineering/comments/1da910n/what_reasons_do_i_have_to_keep_any_data_in/l7opju1/"}
{"body": "Could you elaborate more on why you think Hudi/Paimon aren't going to make it?", "author": "boredconfusedtired", "created_utc": "2024-06-08 15:10:51", "permalink": "/r/dataengineering/comments/1d8118g/databricks_acquires_tabular/l7onmci/"}
{"body": "Try to follow a tutorial but use a different data source, a different orchestrator, etc. I think it's the perfect combination of having an outline of what steps to take and finding stuff out on your own.", "author": "chonbee", "created_utc": "2024-06-08 15:10:41", "permalink": "/r/dataengineering/comments/1dal3z9/how_did_you_find_ideas_for_portfoliopersonal/l7onlfb/"}
{"body": "Overkill to start, but this website: [https://www.redshiftresearchproject.org/white\\_papers/index.html](https://www.redshiftresearchproject.org/white_papers/index.html)\n\nHad every bit of information I could ever need about Redshift.", "author": "m3-bs", "created_utc": "2024-06-08 15:04:28", "permalink": "/r/dataengineering/comments/1db3a6m/redshift_implementation/l7ommyo/"}
{"body": "In my company there is a lot of coding in  SQL (multiple dialects) combined with Python, Ruby, Elixir and sometimes Rust or Go. It depends on the task. But mainly Python with SQL", "author": "headdertz", "created_utc": "2024-06-08 14:50:58", "permalink": "/r/dataengineering/comments/1d9o0sv/how_much_coding_do_data_engineers_do/l7oklck/"}
{"body": "Just one slight modification.  ML is AI.  There has been an explosion of \"AI enabled\" tools out there because everyone's CEOs want \"AI\" so everything they present has to be \"AI\".  Went to a presentation our cloud provider  did for us about using AI methods in our industry and it was basically a simple time series analysis on their ML tools.  I have seen a lot of other references like this too.  Was thinking I should see if I have some old school papers somewhere where I calculated a correlation manually and then I could say I was doing AI in my head long before it was a thing!", "author": "Gators1992", "created_utc": "2024-06-08 14:50:52", "permalink": "/r/dataengineering/comments/1d9s2qe/what_are_everyones_hot_takes_with_some_of_the/l7okktk/"}
{"body": "Moreover, they're often either founded by or quickly hire the \"main\" people who originally developed the open-source tool. Most individual open source tools still have the bulk of the initial commits done by a fairly small group of developers, especially if we're talking about a niche within the data space.", "author": "AnimaLepton", "created_utc": "2024-06-08 14:43:36", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7ojh61/"}
{"body": "They\u2019ll be fast as long as it\u2019s through `COPY INTO`", "author": "Ok_Expert2790", "created_utc": "2024-06-08 14:40:32", "permalink": "/r/dataengineering/comments/1davs19/dynamodb_or_other_options/l7oj0q7/"}
{"body": "What do you think about redshift and snowflake", "author": "learningpundit", "created_utc": "2024-06-08 14:28:24", "permalink": "/r/dataengineering/comments/1davs19/dynamodb_or_other_options/l7oh7kk/"}
{"body": "Very easy. Just install and use.", "author": "RevolutionaryRoyal39", "created_utc": "2024-06-08 14:27:00", "permalink": "/r/dataengineering/comments/1davs19/dynamodb_or_other_options/l7oh033/"}
{"body": "Also, can I use redshift/snowflake instead?", "author": "learningpundit", "created_utc": "2024-06-08 14:18:23", "permalink": "/r/dataengineering/comments/1davs19/dynamodb_or_other_options/l7ofqap/"}
{"body": "Employees only count so much. Unless you\u2019re losing them due to inability to exercise owners don\u2019t care. As far as not going public\u2026you only need to IPO if you want to cash out. A lot of private money isn\u2019t looking to cash out it\u2019s looking to hold and harvest cash. You only IPO if the expected value of the IPO is greater for you than the value of the future cash flows, and that hasn\u2019t apparently been the case recently which is a big reason we\u2019ve seen fewer IPOs.", "author": "AlgoRhythmCO", "created_utc": "2024-06-08 14:15:04", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7of8y4/"}
{"body": "Is it easy to self host clickhouse? Without having to go enterprise?", "author": "learningpundit", "created_utc": "2024-06-08 14:12:37", "permalink": "/r/dataengineering/comments/1davs19/dynamodb_or_other_options/l7oew76/"}
{"body": "Tidious though...", "author": "Moonee_Star", "created_utc": "2024-06-08 14:11:56", "permalink": "/r/dataengineering/comments/1db2fg8/data_flow_diagram_using_mermaid/l7oeslm/"}
{"body": "You can only do that so much until investors and employees grow impatient and what to liquidate. Not going public is just not a long term strategy. They\u2019ve been private for over a decade right?", "author": "Accurate-Peak4856", "created_utc": "2024-06-08 14:10:19", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7oek49/"}
{"body": "I don\u2019t think so if you are licensing them separately and not using the Fabric UI.  Far as I can see OneLake and the fabric UI are what constitutes \u201cfabric\u201d.   Even though they show power bi and DF as inside the fabric boundary on heir diagrams.  \n\nBut this is why I don\u2019t really care for it.  It\u2019s so damned vague and unclear.   And the fabric tier licenses are insanely expensive.  And it\u2019s not even clear what you\u2019re getting vs just paying for some of these tools under azure resources like normal.", "author": "reelznfeelz", "created_utc": "2024-06-08 14:06:55", "permalink": "/r/dataengineering/comments/1dan6w0/are_databricks_really_going_after_snowflake_or_is/l7oe2mt/"}
{"body": "Just draw.io it. Doesn't look super complicated.", "author": "Yoctometre", "created_utc": "2024-06-08 14:04:24", "permalink": "/r/dataengineering/comments/1db2fg8/data_flow_diagram_using_mermaid/l7odpmu/"}
{"body": "That\u2019s a vast oversimplification of why and when firms IPO, especially as IPOs have become less favorable vs just throwing off cash as a private company.", "author": "AlgoRhythmCO", "created_utc": "2024-06-08 14:01:01", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7od8gc/"}
{"body": "Pure growth plays only last for a while. Uber was pure growth since its origin. Then Dara had to come in and clean up shop for 3 years to bring profitability. Databricks isn\u2019t focused on profitability because it is hard to achieve. So, they keep saying growth so investors are happy. If their financials were any good, they would have been public by now.", "author": "Accurate-Peak4856", "created_utc": "2024-06-08 13:50:33", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7obr29/"}
{"body": "Ive ran into similar problem where I had to mix implementations (SQL and dataframe) that I solved using ibis.\u00a0 I used SQL for the simpler large aggregations that you can issue with ibis\u00a0and then used ibis' dataframe api to do the more complex transformations or ones that would be more verbose using sql like doing a pivot wider type of operation which I needed to do dynamically where if i were to do this with SQL I would have had to type a bunch of column names.", "author": "justanothersnek", "created_utc": "2024-06-08 13:44:28", "permalink": "/r/dataengineering/comments/1dacnbs/dataframes_oop_hybrid_setup/l7oawee/"}
{"body": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "author": "AutoModerator", "created_utc": "2024-06-08 13:43:55", "permalink": "/r/dataengineering/comments/1db3a6m/redshift_implementation/l7oatn3/"}
{"body": "Thank you so much!", "author": "Quirky_Agency_246", "created_utc": "2024-06-08 13:40:04", "permalink": "/r/dataengineering/comments/1dad3e7/accepted_as_a_jr_data_engineer/l7oaaos/"}
{"body": "GCP does this. DBT -&gt; Dataform, Airflow -&gt; Cloud Composer, etc.\n\nEdit: Who would dislike this comment?! It\u2019s a direct answer to OP\u2019s question. Dataform is literally Google\u2019s alternative to DBT. Cloud Composer is literally Google\u2019s alternative to Airflow.", "author": "Automatic_Red", "created_utc": "2024-06-08 13:28:40", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7o8ra7/"}
{"body": "What is the appeal of dlt?  It lacks features that vanilla spark provides, doesn't play well with existing databricks products like unity catalog and all purpose clusters and introduces vendor lock in.", "author": "mjgcfb", "created_utc": "2024-06-08 13:25:36", "permalink": "/r/dataengineering/comments/1daj3n0/what_do_you_predict_with_be_announced_at_the/l7o8cfz/"}
{"body": "Kubernetes operator on K8. But if you need a bunch of maven dependencies it just makes more sense to program your spark app in Java. \n\nFor example, sparks out-of-the-box commiter doesn\u2019t work well on S3. You need to use an S3A commiter, but you need to install a million dependencies from maven.", "author": "Old-Personality7953", "created_utc": "2024-06-08 13:20:44", "permalink": "/r/dataengineering/comments/1d9mqmj/pyspark_in_production/l7o7pnf/"}
{"body": "I got access to the serverless runtime a month or so ago.  It's the biggest quality of life improvement databricks has ever released.", "author": "mjgcfb", "created_utc": "2024-06-08 13:19:52", "permalink": "/r/dataengineering/comments/1daj3n0/what_do_you_predict_with_be_announced_at_the/l7o7loy/"}
{"body": "If you work for a company/have a monetary interest in the entity you are promoting you must clearly state your relationship. See more here: https://www.ftc.gov/influencers", "author": "dataengineering-ModTeam", "created_utc": "2024-06-08 13:16:22", "permalink": "/r/dataengineering/comments/1darc9t/what_do_you_think_of_this_data_platform/l7o75eo/"}
{"body": "Your post/comment was removed because it violated rule #3 (Do a search before asking a question). The question you asked has already been answered recently so we remove redundant questions to keep the feed digestable for everyone.", "author": "dataengineering-ModTeam", "created_utc": "2024-06-08 13:14:47", "permalink": "/r/dataengineering/comments/1day664/vs_code_intellisense_for_dbt/l7o6xy1/"}
{"body": "Tell me your environment is misconfigured without telling me your environment is misconfigured.", "author": "sofakingbald", "created_utc": "2024-06-08 13:05:47", "permalink": "/r/dataengineering/comments/1da910n/what_reasons_do_i_have_to_keep_any_data_in/l7o5t0h/"}
{"body": "If you OS with a paid version, the paid version had better make your customers life much easier than the OS version.  And you have to have rock solid support SLAs.  Those are the only two things that you are actually selling.", "author": "joemerchant2021", "created_utc": "2024-06-08 13:02:42", "permalink": "/r/dataengineering/comments/1day6ka/these_companies_offering_a_cloud_premium_version/l7o5f70/"}
{"body": "Serverless python runtime for notebooks, dlt, etc?\nMulti catalog and schema support for dlt?", "author": "SuitCool", "created_utc": "2024-06-08 12:56:46", "permalink": "/r/dataengineering/comments/1daj3n0/what_do_you_predict_with_be_announced_at_the/l7o4on7/"}
{"body": "First off, don\u2019t use Spark. The overhead and the effort of using the clunky API isn\u2019t worth it. \n\nThere are distributed DataFrame libraries that have the mostly the same API as Pandas. Dask is the most commons one. \n\nOn top of that there are libraries like Modin which go even further, creating a yet more complete reimplementation of the Pandas API on a bunch of underlying implementations, including Dask\n\nSo my 16 core laptop with 32GB of RAM, I can replace \n\n    import pandas as pd\n\nWith \n\n    import modin.pandas as pd\n\nAnd my code will transparently run on a locally instantiated 16-process Dask cluster instead. Note that in practice each process probably only has about 1.5GB of memory to work with. \n\nOf course, for an about $1.50/hour you could rent a 32-vCPU m5a instance with 128GB of RAM, which would allow 16 processes with 8GB of RAM each; for $7/hour you can find i4 instances with 512GB of RAM", "author": "budgefrankly", "created_utc": "2024-06-08 12:53:17", "permalink": "/r/dataengineering/comments/1d76o47/duckdb_10_released/l7o496a/"}
{"body": "If they were human, Databricks and Snowflake are practically teenagers, Fabric is 6-months old.", "author": "Data_cruncher", "created_utc": "2024-06-08 12:52:51", "permalink": "/r/dataengineering/comments/1dan6w0/are_databricks_really_going_after_snowflake_or_is/l7o478s/"}
{"body": "I can only speak from experience and it is tiresome and cumbersome to work with.", "author": "Hear7y", "created_utc": "2024-06-08 12:49:32", "permalink": "/r/dataengineering/comments/1dan6w0/are_databricks_really_going_after_snowflake_or_is/l7o3seb/"}
